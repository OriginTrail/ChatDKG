{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NXLv_9hkrIyp",
        "CM3_S7XPrQW0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages and configure GCP"
      ],
      "metadata": {
        "id": "Z3jhxop9qZ9j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfu_0q_8FYm1"
      },
      "outputs": [],
      "source": [
        "%pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade google-cloud-aiplatform"
      ],
      "metadata": {
        "id": "6HRZZpKMj3p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "H2bB6WesFejT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project \"your-project-goes-here\""
      ],
      "metadata": {
        "id": "5k-kNFihFf01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "id": "KYkEvPBELF29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud init"
      ],
      "metadata": {
        "id": "6FGe5iZMHeUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and set up dkg.py, Python SDK for interacting with the DKG"
      ],
      "metadata": {
        "id": "toQQKgwkqryE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade dkg\n",
        "%pip install python-dotenv\n",
        "%pip show dkg"
      ],
      "metadata": {
        "id": "6rxjRVUcdJrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dkg import DKG\n",
        "from dkg.providers import BlockchainProvider, NodeHTTPProvider\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import json\n",
        "\n",
        "dotenv_path = './.env'\n",
        "load_dotenv(dotenv_path)\n",
        "jwt_token = os.getenv('JWT_TOKEN')\n",
        "ot_node_hostname = os.getenv('OT_NODE_HOSTNAME')\n",
        "private_key = os.getenv('PRIVATE_KEY')\n",
        "\n",
        "node_provider = NodeHTTPProvider(ot_node_hostname, jwt_token)\n",
        "blockchain_provider = BlockchainProvider(\"mainnet\", \"otp:2043\", private_key=private_key, gas_price=40)\n",
        "\n",
        "dkg = DKG(node_provider, blockchain_provider)\n",
        "print(dkg.node.info)"
      ],
      "metadata": {
        "id": "RSBX8bIX8oRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instruct Gemini to create a Knowledge Asset on the DKG"
      ],
      "metadata": {
        "id": "B7LooDwWguLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview.generative_models import GenerativeModel, ChatSession, Content, Part, GenerationConfig\n",
        "\n",
        "def get_chat_response(chat: ChatSession, prompt: str):\n",
        "  response = chat.send_message(prompt, generation_config=GenerationConfig(temperature=0))\n",
        "  return response.candidates[0].content.parts[0].text\n",
        "\n",
        "\n",
        "instruction_message = '''\n",
        "Construct a JSON object following the artwork JSON-LD schema based on the provided information by the user.\n",
        "The user will provide the image URL, artwork name, description, artform, author name. For the publisher name you can assume 'Google Demo Amsterdam'.\n",
        "Add 5 keywords based on the artwork name and description.\n",
        "Use the provided image URL for the id field as well.\n",
        "\n",
        "Here's an example of an artwork that corresponds to the mentioned JSON-LD schema:\n",
        "{\n",
        "  \"@context\": \"http://schema.org\",\n",
        "  \"@type\": \"VisualArtwork\",\n",
        "  \"@id\": \"https://origintrail.io/images/otworld/1fc7cb79f299ee4.jpg\",\n",
        "  \"name\": \"The Last Supper\",\n",
        "  \"description\": \"The Last Supper is an iconic Renaissance fresco by Leonardo Da Vinci.\",\n",
        "  \"artform\": \"Painting\",\n",
        "  \"author\": { \"@type\": \"Person\", \"name\": \"Leonardo da Vinci\" },\n",
        "  \"image\": \"https://origintrail.io/images/otworld/1fc7cb79f299ee4.jpg\",\n",
        "  \"keywords\": [ \"The Last Supper\", \"Leonardo da Vinci\", \"Renaissance\", \"fresco\", \"religious art\" ],\n",
        "  \"publisher\": { \"@type\": \"Person\", \"name\": \"dkgbrka\" }\n",
        "}\n",
        "\n",
        "Output the JSON as a string, between ```json and ```.\n",
        "'''\n",
        "\n",
        "instruction_understood_message = \"Yes.\"\n",
        "\n",
        "\n",
        "chat_history = [\n",
        "    Content(parts=[Part.from_text(instruction_message)], role=\"user\"),\n",
        "    Content(parts=[Part.from_text(instruction_understood_message)], role=\"model\"),\n",
        "]"
      ],
      "metadata": {
        "id": "82K3DXFmgX9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_pro_model = GenerativeModel(\"gemini-1.0-pro-001\", generation_config=GenerationConfig(temperature=0))\n",
        "chat = gemini_pro_model.start_chat(history=chat_history)\n",
        "question = '''Create an artwork with name: Decentralized Hexagon, created by OriginTrail. Description should be \"Digital art showcasing the OriginTrail network in an artistic way\" and you can conclude the artform yourself. Here's the image URL https://i.gyazo.com/72b6cd16e0d5b2e131b0311456dcdefc.png'''\n",
        "generated_json = get_chat_response(chat, question)\n",
        "\n",
        "print(generated_json)"
      ],
      "metadata": {
        "id": "x8onbk-viVVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_json_string(input_string):\n",
        "    if input_string.startswith(\"```json\") and input_string.endswith(\"```\"):\n",
        "        cleaned_query = input_string[7:-3].strip()\n",
        "        return cleaned_query\n",
        "    elif input_string.startswith(\"```\") and input_string.endswith(\"```\"):\n",
        "        cleaned_query = input_string[3:-3].strip()\n",
        "    else:\n",
        "        return input_string"
      ],
      "metadata": {
        "id": "eK7gks7-lyO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "artwork = json.loads(clean_json_string(generated_json))\n",
        "\n",
        "content = {\"public\": artwork}\n",
        "create_asset_result = dkg.asset.create(content, 2)\n",
        "print('Asset created!')\n",
        "print(json.dumps(create_asset_result, indent=4))\n",
        "print(create_asset_result[\"UAL\"])\n",
        "\n",
        "get_asset_result = dkg.asset.get(create_asset_result[\"UAL\"])\n",
        "print(json.dumps(get_asset_result, indent=4))"
      ],
      "metadata": {
        "id": "2mpuX1wKkymp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instruct Gemini to construct a SparQL query in order to query the DKG"
      ],
      "metadata": {
        "id": "1w9am_BRqiAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview.generative_models import GenerativeModel, ChatSession, Content, Part, GenerationConfig\n",
        "\n",
        "def get_chat_response(chat: ChatSession, prompt: str) -> str:\n",
        "    response = chat.send_message(prompt, generation_config=GenerationConfig(temperature=0))\n",
        "    print(response)\n",
        "\n",
        "    return response.candidates[0].content.parts[0].text\n",
        "\n",
        "instruction_message = '''\n",
        "I am working on a project involving artworks and their related data. I have a schema in JSON-LD format that outlines the structure and relationships of the data I am dealing with. Based on this schema, I need to construct a SPARQL query to retrieve specific information from a dataset that follows this schema.\n",
        "\n",
        "The schema is focused on artworks and includes various properties such as the artist, description, artform and author among others. My goal with the SPARQL queries is to retrieve data from the graph about the artworks, based on the natural language question that the user posed.\n",
        "\n",
        "Here's an example of an artwork the JSON-LD format: { \"@context\": \"http://schema.org\", \"@type\": \"VisualArtwork\", \"@id\": \"https://origintrail.io/images/otworld/1fc7cb79f299ee4.jpg\", \"name\": \"The Last Supper\", \"description\": \"The Last Supper is an iconic Renaissance fresco by Leonardo Da Vinci.\", \"artform\": \"Painting\", \"author\": { \"@type\": \"Person\", \"name\": \"Leonardo da Vinci\" }, \"image\": \"https://origintrail.io/images/otworld/1fc7cb79f299ee4.jpg\", \"keywords\": [ \"The Last Supper\", \"Leonardo da Vinci\", \"Renaissance\", \"fresco\", \"religious art\" ], \"publisher\": { \"@type\": \"Person\", \"name\": \"dkgbrka\" } }\n",
        "\n",
        "Here's an example of a query to find artworks from publisher \"BranaRakic\":\n",
        "```sparql\n",
        "PREFIX schema: <http://schema.org/>\n",
        "\n",
        "SELECT ?artwork ?name ?ual\n",
        "\n",
        "WHERE { ?artwork a schema:VisualArtwork ;\n",
        "GRAPH ?g\n",
        "{ ?artwork schema:publisher/schema:name \"BranaRakic\" ; schema:name ?name . }\n",
        "\n",
        "?ual schema:assertion ?g\n",
        "FILTER(CONTAINS(str(?ual), \"2043\")) }```\n",
        "\n",
        "Pay attention to retrieving the UAL, this is a mandatory step of all your queries. After getting the artwork with '?artwork a schema:VisualArtwork ;' you should wrap the next conditions around GRAPH ?g { }, and later use the graph retrieved (g) to get the UAL like in the example above.\n",
        "\n",
        "Make sure you ALWAYS retrieve the UAL no matter what the user asks for and filter whether it contains \"2043\".\n",
        "\n",
        "Make sure you ONLY return the SparQL query without any extra output.\n",
        "\n",
        "If you understood the assignment, say 'Yes' and I will proceed with a natural language question which you should convert to a SparQL query.'''\n",
        "\n",
        "instruction_understood_message = \"Yes.\"\n",
        "\n",
        "\n",
        "chat_history = [\n",
        "    Content(parts=[Part.from_text(instruction_message)], role=\"user\"),\n",
        "    Content(parts=[Part.from_text(instruction_understood_message)], role=\"model\"),\n",
        "]"
      ],
      "metadata": {
        "id": "HuuR6DFjLhpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_sparql_query(input_string):\n",
        "    if input_string.startswith(\"```sparql\") and input_string.endswith(\"```\"):\n",
        "        cleaned_query = input_string[9:-3].strip()\n",
        "        return cleaned_query\n",
        "    elif input_string.startswith(\"```\") and input_string.endswith(\"```\"):\n",
        "        cleaned_query = input_string[3:-3].strip()\n",
        "    else:\n",
        "        return input_string\n",
        "\n",
        "gemini_pro_model = GenerativeModel(\"gemini-1.0-pro-001\", generation_config=GenerationConfig(temperature=0))\n",
        "chat = gemini_pro_model.start_chat(history=chat_history)\n",
        "question = \"Provide me with all the artworks published by Google Demo Amsterdam\"\n",
        "print(get_chat_response(chat, question))\n",
        "query = clean_sparql_query(get_chat_response(chat, question))\n",
        "\n",
        "print(query)"
      ],
      "metadata": {
        "id": "njYQCxnjqZL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use dkg.py to query the DKG"
      ],
      "metadata": {
        "id": "vErJh7Afq6Fd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_result = dkg.graph.query(query, \"privateCurrent\")\n",
        "print(\"======================== QUERY LOCAL CURRENT RESULT\")\n",
        "print(query_result)"
      ],
      "metadata": {
        "id": "-wKF8sp8c0wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decentralized Retrieval Based Generation (dRAG) based on the query results"
      ],
      "metadata": {
        "id": "xdwCVEZxq-rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_results = \"\\n\".join([f\"- Title: {artwork['name']}, UAL: {artwork['ual']}\" for artwork in query_result])\n",
        "\n",
        "prompt = (\n",
        "  f\"I have retrieved the following information from the Decentralized Knowledge Graph based on the query '{query}':\\n\"\n",
        "  f\"{formatted_results}\\n\\n\"\n",
        "  \"Imagine you're guiding a tour through a virtual gallery featuring some of the most iconic artworks linked to detailed records in the Decentralized Knowledge Graph.\\n\"\n",
        "  \"As you introduce these artworks to the audience, delve into the stories behind them. What inspired these pieces? How do they reflect the emotions and techniques of the artist?\\n\"\n",
        "  f\"Question: {question}\\n\"\n",
        "  \"Answer:\"\n",
        ")\n",
        "\n",
        "llm_response = gemini_pro_model.generate_content(prompt)\n",
        "print(llm_response)"
      ],
      "metadata": {
        "id": "Z065b6sq9bLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini Vision Demo\n",
        "\n"
      ],
      "metadata": {
        "id": "NXLv_9hkrIyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install requests google-cloud-storage"
      ],
      "metadata": {
        "id": "6iMXBYsMJMhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from google.cloud import storage\n",
        "import datetime\n",
        "\n",
        "def generate_signed_url(image_url, bucket_name, destination_blob_name, expiration=3600):\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    response = requests.get(image_url)\n",
        "    if response.status_code == 200:\n",
        "        # Upload the image to GCS\n",
        "        blob.upload_from_string(response.content, content_type=response.headers['Content-Type'])\n",
        "        print(f\"File {destination_blob_name} uploaded to {bucket_name}.\")\n",
        "\n",
        "        return f\"gs://{bucket_name}/{destination_blob_name}\"\n",
        "    else:\n",
        "        print(f\"Failed to fetch image from URL: {image_url}\")"
      ],
      "metadata": {
        "id": "-yIbIE7sJOiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview.generative_models import GenerativeModel\n",
        "from vertexai.preview.generative_models import Part\n",
        "import uuid\n",
        "\n",
        "gemini_pro_vision_model = GenerativeModel(\"gemini-1.0-pro-vision\")\n",
        "\n",
        "for artpiece in query_result:\n",
        "  try:\n",
        "    image_url = generate_signed_url(artpiece[\"artwork\"], \"amsterdam-public-bucket\", str(uuid.uuid4())+\".jpg\")\n",
        "    image = Part.from_uri(image_url, mime_type=\"image/jpeg\")\n",
        "    model_response = gemini_pro_vision_model.generate_content([\"Examine the image and provide a comprehensive description, highlighting its contents, subjects, objects, and overall setting.\", image])\n",
        "    artpiece[\"description\"] = model_response.candidates[0].content.parts[0].text\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    artpiece[\"description\"] = \"An error occurred while generating the description.\"\n"
      ],
      "metadata": {
        "id": "9Q_t6mPyHWXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use in memory vector store\n",
        "%pip install annoy"
      ],
      "metadata": {
        "id": "CfilmC-Nerh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector search"
      ],
      "metadata": {
        "id": "CM3_S7XPrQW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.language_models import TextEmbeddingModel\n",
        "from annoy import AnnoyIndex\n",
        "\n",
        "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko-multilingual@001\")\n",
        "\n",
        "def build_embeddings_index(embeddings, n_trees=10):\n",
        "    dim = len(embeddings[0])\n",
        "    index = AnnoyIndex(dim, 'angular')  # Using angular distance\n",
        "\n",
        "    for i, vector in enumerate(embeddings):\n",
        "        index.add_item(i, vector)\n",
        "\n",
        "    index.build(n_trees)\n",
        "    return index\n",
        "\n",
        "def add_text_embeddings(artworks):\n",
        "    for artwork in artworks:\n",
        "        embeddings = model.get_embeddings([artwork[\"description\"]])\n",
        "        artwork[\"embedding\"] = embeddings[0].values\n",
        "\n",
        "add_text_embeddings(query_result)\n",
        "index = build_embeddings_index([artwork[\"embedding\"] for artwork in query_result])\n",
        "question = \"I would like to buy a painting which depicts an angel android in a cyber-punk style.\"\n",
        "\n",
        "nearest_neighbors = index.get_nns_by_vector(model.get_embeddings([question])[0].values, 1, include_distances=True)\n",
        "index_of_nearest_neighbor = nearest_neighbors[0][0]\n",
        "\n",
        "print(f\"Vector search result: {query_result[index_of_nearest_neighbor]['description']}\")\n",
        "print(f\"Painting name: {query_result[index_of_nearest_neighbor]['name']}\")\n",
        "print(f\"https://dkg.origintrail.io/explore?ual={query_result[index_of_nearest_neighbor]['ual']}\")\n"
      ],
      "metadata": {
        "id": "OZafhEC2MfOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarking very basic RAG vs KG dRAG"
      ],
      "metadata": {
        "id": "EkR9VGv5riHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = '''PREFIX schema: <http://schema.org/>\n",
        "\n",
        "SELECT ?artwork ?name ?ual\n",
        "\n",
        "WHERE { ?artwork a schema:VisualArtwork .\n",
        "GRAPH ?g\n",
        "{ ?artwork schema:name ?name . }\n",
        "\n",
        "?ual schema:assertion ?g\n",
        "FILTER(CONTAINS(str(?ual), \"2043\")) }\n",
        "'''\n",
        "\n",
        "query_result = dkg.graph.query(query, \"privateCurrent\")\n",
        "print(\"======================== QUERY LOCAL CURRENT RESULT\")\n",
        "\n",
        "formatted_results = \"\\n\".join([f\"- Title: {artwork['name']}, UAL: {artwork['ual']}\" for artwork in query_result])\n",
        "\n",
        "prompt = (\n",
        "  f\"I have retrieved the following information from the Decentralized Knowledge Graph based on the query '{query}':\\n\"\n",
        "  f\"{formatted_results}\\n\\n\"\n",
        "  \"Imagine you're guiding a tour through a virtual gallery featuring some of the most iconic artworks linked to detailed records in the Decentralized Knowledge Graph.\\n\"\n",
        "  \"As you introduce these artworks to the audience, delve into the stories behind them. What inspired these pieces? How do they reflect the emotions and techniques of the artist? Discuss the significance of each artwork, considering its historical context, the artist's life at the time, and its impact on art history.\\n\"\n",
        "  f\"Question: {question}\\n\"\n",
        "  \"Answer:\"\n",
        ")\n",
        "\n",
        "llm_response = gemini_pro_model.generate_content(prompt)\n",
        "print(llm_response)"
      ],
      "metadata": {
        "id": "cvXNUUtTG157"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}